import { ref, computed } from 'vue'
import { defineStore } from 'pinia'
import { ChatStorage } from '@/utils/storage'
import { websocketService } from '@/services/websocket'
import { audioRecordingService } from '@/services/audioRecording'
import { audioPlaybackService } from '@/services/audioPlayback'
import type { ConnectionState, ListenMode, TTSMessage, LLMMessage } from '@/types/websocket'
import { errorHandler } from '@/utils/errorHandler'

export interface ChatMessage {
  id: string
  text: string
  isUser: boolean
  timestamp: Date
  audioUrl?: string
  emotion?: string
}

export interface VoiceSettings {
  lang: string
  pitch: number
  rate: number
  volume: number
}

export const useChatStore = defineStore('chat', () => {
  // æ¶ˆæ¯åˆ—è¡¨
  const messages = ref<ChatMessage[]>([])

  // Websocket è¿æ¥çŠ¶æ€ (use service state directly to avoid sync issues)
  const connectionState = computed(() => websocketService.getConnectionState())
  const isConnected = computed(() => websocketService.isConnected())

  // è¯­éŸ³è¯†åˆ«è®¾ç½®
  const isListening = ref(false)
  const listenMode = ref<ListenMode>('auto')
  const recognitionLang = ref('zh-CN')

  // è¯­éŸ³åˆæˆè®¾ç½®
  const voiceSettings = ref<VoiceSettings>({
    lang: 'zh-CN',
    pitch: 1,
    rate: 1,
    volume: 1
  })

  // å½“å‰è¾“å…¥
  const currentInput = ref('')

  // æ˜¯å¦æ­£åœ¨æ’­æ”¾è¯­éŸ³
  const isSpeaking = ref(false)

  // å½“å‰æƒ…æ„ŸçŠ¶æ€
  const currentEmotion = ref<string>('normal')

  // TTS æ–‡æœ¬ç¼“å­˜
  const currentTTSText = ref<string>('')

  // è·å–æœ€æ–°æ¶ˆæ¯
  const latestMessage = computed(() => messages.value[messages.value.length - 1])

  // è·å–ç»Ÿè®¡ä¿¡æ¯
  const stats = computed(() => ChatStorage.getHistoryStats(messages.value))

  // åˆå§‹åŒ–æ—¶åŠ è½½å†å²è®°å½•
  function loadHistory() {
    const history = ChatStorage.loadHistory()
    messages.value = history
  }

  // ä¿å­˜åˆ°å†å²è®°å½•
  function saveHistory() {
    ChatStorage.saveHistory(messages.value)
  }

  // æ·»åŠ æ¶ˆæ¯
  function addMessage(text: string, isUser: boolean = true, audioUrl?: string, emotion?: string) {
    const message: ChatMessage = {
      id: Date.now().toString(),
      text,
      isUser,
      timestamp: new Date(),
      audioUrl,
      emotion
    }
    messages.value.push(message)

    // ä¿å­˜åˆ°æœ¬åœ°å­˜å‚¨
    saveHistory()

    return message
  }

  // æ¸…ç©ºæ¶ˆæ¯
  function clearMessages() {
    messages.value = []
    ChatStorage.clearHistory()
  }

  // è®¾ç½®è¾“å…¥
  function setInput(text: string) {
    currentInput.value = text
  }

  // æ¸…é™¤è¾“å…¥
  function clearInput() {
    currentInput.value = ''
  }

  // æ›´æ–°è¯­éŸ³è®¾ç½®
  function updateVoiceSettings(settings: Partial<VoiceSettings>) {
    voiceSettings.value = { ...voiceSettings.value, ...settings }
  }

  // è®¾ç½®è¯­éŸ³è¯†åˆ«çŠ¶æ€
  function setListening(listening: boolean) {
    isListening.value = listening
  }

  // è®¾ç½®è¯­éŸ³æ’­æ”¾çŠ¶æ€
  function setSpeaking(speaking: boolean) {
    isSpeaking.value = speaking
  }

  // è®¾ç½®æƒ…æ„ŸçŠ¶æ€
  function setEmotion(emotion: string) {
    currentEmotion.value = emotion
  }

  // å¯¼å‡ºèŠå¤©è®°å½•
  function exportHistory(): string {
    return ChatStorage.exportHistory(messages.value)
  }

  // å¯¼å…¥èŠå¤©è®°å½•
  function importHistory(jsonString: string): boolean {
    try {
      const importedMessages = ChatStorage.importHistory(jsonString)
      if (importedMessages.length > 0) {
        messages.value = importedMessages
        saveHistory()
        return true
      }
    }
    catch (error) {
      console.warn('å¯¼å…¥èŠå¤©è®°å½•å¤±è´¥:', error)
    }
    return false
  }

  // ==== Websocket Integration ====

  /**
   * Connect to Websocket server
   */
  async function connectWebsocket(): Promise<void> {
    try {
      // Initialize services
      await audioRecordingService.initialize()
      await audioPlaybackService.initialize()

      // Setup event handlers and connect
      await websocketService.connect({
        onConnected: handleWebsocketConnected,
        onDisconnected: handleWebsocketDisconnected,
        onTTS: handleTTSMessage,
        onLLM: handleLLMMessage,
        onAudioData: handleAudioData,
        onError: handleWebsocketError
      })

      errorHandler.showSuccess('è¿æ¥æˆåŠŸ', 'Websocketè¿æ¥å·²å»ºç«‹')
    }
    catch (error) {
      const errorMessage = error instanceof Error ? error.message : 'Unknown error'
      errorHandler.showErrorMessage('è¿æ¥å¤±è´¥', errorMessage)
      throw error
    }
  }

  /**
   * Disconnect from Websocket server
   */
  function disconnectWebsocket(): void {
    websocketService.disconnect()
    audioRecordingService.dispose()
    audioPlaybackService.dispose()
  }

  /**
   * Start voice listening
   */
  async function startVoiceListen(mode: ListenMode = 'auto'): Promise<void> {
    if (!isConnected.value) {
      errorHandler.showErrorMessage('æœªè¿æ¥', 'è¯·å…ˆè¿æ¥åˆ°æœåŠ¡å™¨')
      return
    }

    try {
      // Start recording (now async with OPUS encoder initialization)
      await audioRecordingService.startRecording((audioData) => {
        websocketService.sendAudioData(audioData)
      })

      // Send listen start message
      websocketService.startListen(mode)
      listenMode.value = mode
      isListening.value = true

      console.log('ğŸ¤ Started voice listening')
    }
    catch (error) {
      const errorMessage = error instanceof Error ? error.message : 'Unknown error'
      errorHandler.showErrorMessage('å¯åŠ¨å½•éŸ³å¤±è´¥', errorMessage)
      throw error
    }
  }

  /**
   * Stop voice listening
   */
  function stopVoiceListen(): void {
    audioRecordingService.stopRecording()
    websocketService.stopListen()
    isListening.value = false

    console.log('ğŸ›‘ Stopped voice listening')
  }

  /**
   * Handle Websocket connected event
   */
  function handleWebsocketConnected(): void {
    console.log('âœ… Websocket connected')
  }

  /**
   * Handle Websocket disconnected event
   */
  function handleWebsocketDisconnected(): void {
    console.log('ğŸ”Œ Websocket disconnected')
    isListening.value = false
    isSpeaking.value = false
  }

  /**
   * Handle TTS message from server
   */
  function handleTTSMessage(message: TTSMessage): void {
    console.log('ğŸ”Š TTS message:', message.state, message.text)

    if (message.state === 'sentence_start' && message.text) {
      // Add AI response message
      currentTTSText.value = message.text
      addMessage(message.text, false, undefined, currentEmotion.value)
    }
    else if (message.state === 'start') {
      isSpeaking.value = true
      audioPlaybackService.setCallbacks(
        () => isSpeaking.value = true,
        () => isSpeaking.value = false
      )
    }
    else if (message.state === 'stop') {
      isSpeaking.value = false
      currentTTSText.value = ''
    }
  }

  /**
   * Handle LLM emotion message from server
   */
  function handleLLMMessage(message: LLMMessage): void {
    console.log('ğŸ˜Š LLM emotion:', message.emotion)
    setEmotion(message.emotion)
  }

  /**
   * Handle audio data from server
   */
  function handleAudioData(data: ArrayBuffer): void {
    console.log(`ğŸµ Received audio: ${data.byteLength} bytes`)
    audioPlaybackService.addAudioData(data)
  }

  /**
   * Handle Websocket error
   */
  function handleWebsocketError(error: Error): void {
    console.error('âŒ Websocket error:', error)
    errorHandler.showErrorMessage('Websocketé”™è¯¯', error.message)
  }

  // ==== End Websocket Integration ====

  return {
    // çŠ¶æ€
    messages,
    isListening,
    listenMode,
    recognitionLang,
    voiceSettings,
    currentInput,
    isSpeaking,
    currentEmotion,
    currentTTSText,
    latestMessage,
    stats,
    connectionState,
    isConnected,

    // åŸºç¡€æ–¹æ³•
    loadHistory,
    saveHistory,
    addMessage,
    clearMessages,
    setInput,
    clearInput,
    updateVoiceSettings,
    setListening,
    setSpeaking,
    setEmotion,
    exportHistory,
    importHistory,

    // Websocket æ–¹æ³•
    connectWebsocket,
    disconnectWebsocket,
    startVoiceListen,
    stopVoiceListen
  }
})